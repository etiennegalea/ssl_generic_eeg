{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TUAR data loading, referencing, mapping and annotating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import sys\n",
    "import os\n",
    "import importlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from braindecode.preprocessing.preprocess import preprocess, Preprocessor, zscore\n",
    "from braindecode.datasets import (create_from_mne_raw, create_from_mne_epochs)\n",
    "from braindecode.preprocessing.windowers import create_windows_from_events\n",
    "from braindecode.datasets.sleep_physionet import SleepPhysionet\n",
    "from braindecode.datasets import BaseDataset, BaseConcatDataset, WindowsDataset\n",
    "from mne_extras import write_edf\n",
    "\n",
    "from plot import Plot\n",
    "from segment import Segmenter\n",
    "from helper_funcs import HelperFuncs as hf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mat73 import loadmat\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pprint\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# annotation map\n",
    "REC_MAP = {\n",
    "    0: 'null', 1: 'spsw', 2: 'gped', 3: 'pled', 4: 'eyeb', 5: 'artf',\n",
    "    6: 'bckg', 7: 'seiz', 8: 'fnsz', 9: 'gnsz', 10: 'spsz', 11: 'cpsz',\n",
    "    12: 'absz', 13: 'tnsz', 14: 'cnsz', 15: 'tcsz', 16: 'atsz', 17: 'mysz',\n",
    "    18: 'nesz', 19: 'intr', 20: 'slow', 21: 'eyem', 22: 'chew', 23: 'shiv',\n",
    "    24: 'musc', 25: 'elpp', 26: 'elst', 27: 'calb', 28: 'hphs', 29: 'trip',\n",
    "    30: 'elec', 100: 'eyem_chew', 101: 'eyem_shiv', 102: 'eyem_musc', 103: 'eyem_elec', \n",
    "    104: 'chew_shiv', 105: 'chew_musc', 106: 'chew_elec', 107: 'shiv_musc', 108: 'shiv_elec',\n",
    "    109: 'musc_elec'\n",
    "}\n",
    "\n",
    "\n",
    "# EEG channel montage map for TUAR\n",
    "MONTAGE_MAP = {\n",
    "    0: 'Fp1-F7', # EEG FP1-REF -- EEG F7-REF\n",
    "    # 1: 'F7-T3', # EEG F7-REF -- EEG T3-REF\n",
    "    # 2: 'T3-T5', # EEG T3-REF -- EEG T5-REF\n",
    "    # 3: 'T5-O1', # EEG T5-REF -- EEG O1-REF\n",
    "    # 4: 'Fp2-F8', # EEG FP2-REF -- EEG F8-REF\n",
    "    # 5: 'F8-T4', # EEG F8-REF -- EEG T4-REF\n",
    "    # 6: 'T4-T6', # EEG T4-REF -- EEG T6-REF\n",
    "    # 7: 'T6-O2', # EEG T6-REF -- EEG O2-REF\n",
    "    # # 8: 'A1-T3', # EEG A1-REF -- EEG T3-REF\n",
    "    # 9: 'T3-C3', # EEG T3-REF -- EEG C3-REF\n",
    "    # 10: 'C3-Cz', # EEG C3-REF -- EEG CZ-REF\n",
    "    # 11: 'Cz-C4', # EEG CZ-REF -- EEG C4-REF\n",
    "    # 12: 'C4-T4', # EEG C4-REF -- EEG T4-REF\n",
    "    # # 13: 'T4-A2', # EEG T4-REF -- EEG A2-REF\n",
    "    # 14: 'Fp1-F3', # EEG FP1-REF -- EEG F3-REF\n",
    "    # 15: 'F3-C3', # EEG F3-REF -- EEG C3-REF\n",
    "    # 16: 'C3-P3', # EEG C3-REF -- EEG P3-REF\n",
    "    # 17: 'P3-O1', # EEG P3-REF -- EEG O1-REF\n",
    "    # 18: 'Fp2-F4', # EEG FP2-REF -- EEG F4-REF\n",
    "    # 19: 'F4-C4', # EEG F4-REF -- EEG C4-REF\n",
    "    # 20: 'C4-P4', # EEG C4-REF -- EEG P4-REF\n",
    "    21: 'P4-O2', # EEG P4-REF -- EEG O2-REF\n",
    "}\n",
    "\n",
    "TUAR_EXCLUDE_LIST = ['EEG T3-REF', 'EEG T5-REF', 'EEG F3-REF', 'EEG C3-REF', 'EEG P3-REF',\n",
    "                'EEG O1-REF', 'EEG FP2-REF', 'EEG F8-REF', 'EEG T4-REF', 'EEG T6-REF', 'EEG F4-REF',\n",
    "                'EEG C4-REF', 'EEG FZ-REF', 'EEG CZ-REF', 'EEG PZ-REF',\n",
    "                'EEG ROC-REF', 'EEG LOC-REF', 'EEG EKG1-REF', 'EEG T1-REF', 'EEG T2-REF', 'PHOTIC-REF',\n",
    "                'IBI', 'BURSTS', 'SUPPR']\n",
    "\n",
    "# Standard 10-20 alphabetic channel names\n",
    "STANDARD_10_20 = ['Fp1', 'F7', 'T3', 'T5', 'F3', 'C3', 'P3', 'O1', 'Fp2', 'F8', 'T4', 'T6', 'F4', 'C4',\n",
    "                 'P4', 'O2', 'Fz', 'Cz', 'Pz']\n",
    "# TUAR 10-20 channel names\n",
    "TUAR_CHANNELS = ['EEG FP1-REF', 'EEG F7-REF', 'EEG T3-REF', 'EEG T5-REF', 'EEG F3-REF', 'EEG C3-REF', 'EEG P3-REF',\n",
    "                 'EEG O1-REF', 'EEG FP2-REF', 'EEG F8-REF', 'EEG T4-REF', 'EEG T6-REF', 'EEG F4-REF',\n",
    "                 'EEG C4-REF', 'EEG P4-REF', 'EEG O2-REF', 'EEG FZ-REF', 'EEG CZ-REF', 'EEG PZ-REF']\n",
    "# Non-EEG channels to exclude\n",
    "exclude_channels = ['EEG ROC-REF', 'EEG LOC-REF', 'EEG EKG1-REF', 'EEG T1-REF', 'EEG T2-REF', 'PHOTIC-REF',\n",
    "                    'IBI', 'BURSTS', 'SUPPR']\n",
    "# Mapping of TUAR channels to standard 10-20 channels\n",
    "MAPPING_TUAR_STANDARD_10_20 = dict(zip(TUAR_CHANNELS, STANDARD_10_20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_list(x):\n",
    "    return [os.path.join(x, fname) for fname in os.listdir(x)]\n",
    "\n",
    "def get_id(x):\n",
    "    return x.split('/')[-1]\n",
    "\n",
    "# get TUAR annotations from txt (.rec) file\n",
    "def get_tuar_annotations(txt):\n",
    "    with open(txt, \"r\") as f:\n",
    "        annotations = []\n",
    "        for l in f.readlines():\n",
    "            annot = l.rstrip().split(',')\n",
    "            # select only first and last EEG channels (Fp1-F7, P4-O2)\n",
    "            if int(annot[0]) in MONTAGE_MAP.keys():\n",
    "                annot[0] = MONTAGE_MAP[int(annot[0])]\n",
    "                annot[-1] = REC_MAP[int(annot[-1])]\n",
    "                annotations.append(annot)\n",
    "    # pp.pprint(annotations)\n",
    "    return annotations\n",
    "\n",
    "# Rename channels according to standard montage and map\n",
    "def create_eeg_montage(raw):\n",
    "    # Exclude non-EEG channels\n",
    "    channels = [ch for ch in raw.ch_names if ch not in exclude_channels]\n",
    "    raw.pick_channels(channels)\n",
    "\n",
    "    raw.pick_channels(TUAR_CHANNELS, ordered=True)\n",
    "    # Rename channels to standard 10-20 alphabetic\n",
    "    print('Renaming to standard 10-20 alphabetic channels ...')\n",
    "    # print(STANDARD_10_20)\n",
    "    mne.rename_channels(raw.info, MAPPING_TUAR_STANDARD_10_20)\n",
    "    # Make standard montage\n",
    "    montage = mne.channels.make_standard_montage('standard_alphabetic')\n",
    "    raw.set_montage(montage)\n",
    "\n",
    "    return raw\n",
    "\n",
    "# rereference channels according to TUAR montage\n",
    "def rereference_channels(raw):\n",
    "    data = []\n",
    "    for k, v in MONTAGE_MAP.items():\n",
    "        ch1, ch2 = v.split('-')\n",
    "        x = raw[ch1][0] - raw[ch2][0]\n",
    "        data.append(x[0])\n",
    "    return np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_dir = '/media/maligan/My Passport/msc_thesis/data/tuar/v2_1_0/edf/01_tcp_ar/'\n",
    "\n",
    "files = []\n",
    "descriptions = []\n",
    "error = []\n",
    "\n",
    "subjects = {}\n",
    "for subject in hf.get_file_list(data_dir):\n",
    "    recordings = {}\n",
    "    for recording in hf.get_file_list(subject):\n",
    "        dates = {}\n",
    "        for date in hf.get_file_list(recording):\n",
    "            for raw_path in hf.get_file_list(date):\n",
    "                # print(hf.get_id(raw_path))\n",
    "                annotations = []\n",
    "                if '.rec' in hf.get_id(raw_path) and not '.rec_orig' in hf.get_id(raw_path):\n",
    "                    # annotation file\n",
    "                    # print(f'Getting ANNOTATIONS from: {raw_path}')\n",
    "                    annotations = get_tuar_annotations(raw_path)\n",
    "                    # print(annotations)\n",
    "                if '.edf' in hf.get_id(raw_path):\n",
    "                    # raw file\n",
    "                    # print(f'Getting RAW EDF from: {raw_path}')\n",
    "                    raw = mne.io.read_raw_edf(raw_path)\n",
    "                    raw = create_eeg_montage(raw)\n",
    "                    data = rereference_channels(raw)\n",
    "                    \n",
    "                    info = mne.create_info([MONTAGE_MAP[0], MONTAGE_MAP[21]], ch_types=['eeg']*2, sfreq=raw.info['sfreq'])\n",
    "                    raw = mne.io.RawArray(data, info)\n",
    "                \n",
    "                if annotations:\n",
    "                    # transpose annotation list\n",
    "                    annots = list(zip(*annotations))\n",
    "                    # calculate durations\n",
    "                    # print(durations)\n",
    "                    durations = np.array(annots[2]).astype(float) - np.array(annots[1]).astype(float)\n",
    "                    # set annotations per channel\n",
    "                    raw = raw.set_annotations(mne.Annotations(onset=annots[1], duration=durations, description=annots[3], ch_names=[[x] for x in annots[0]]))\n",
    "\n",
    "            to_export = raw_path.split('.')[0].split('/')[-1]\n",
    "            try:\n",
    "                raw.save(f\"/media/maligan/My Passport/msc_thesis/data/tuar/v2_1_0/processed/{to_export}_2_channels.fif\", picks=['Fp1-F7', 'P4-O2'])\n",
    "            except:\n",
    "                error.append(to_export)\n",
    "\n",
    "            pp.pprint(error)\n",
    "\n",
    "            descriptions += [{'subject': subject, 'recording': recording, 'date': date}]\n",
    "            dates[hf.get_id(date)] = raw\n",
    "        recordings[hf.get_id(recording)] = dates\n",
    "    subjects[hf.get_id(subject)] = recordings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from /media/maligan/My Passport/msc_thesis/data/tuar/v2.1.0/edf/01_tcp_ar/107/00010748/s001_2013_09_19/00010748_s001_t000.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Renaming to standard 10-20 alphabetic channels ...\n",
      "Creating RawArray with float64 data, n_channels=2, n_times=370500\n",
      "    Range : 0 ... 370499 =      0.000 ...  1481.996 secs\n",
      "Ready.\n"
     ]
    }
   ],
   "source": [
    "# # TUAR sample\n",
    "# raw_folder = './data/sample_TUAR/s005_2010_11_15/'\n",
    "# edf_f = '00000254_s005_t000.edf'\n",
    "# annot_txt = '00000254_s005_t000.rec'\n",
    "\n",
    "# raw_path = raw_folder + edf_f\n",
    "\n",
    "# files = []\n",
    "\n",
    "# for raw_path in hf.get_file_list(date):\n",
    "#     if '.rec' in hf.get_id(raw_path) and not '.rec_orig' in hf.get_id(raw_path):\n",
    "#         # annotation file\n",
    "#         annotations = get_tuar_annotations(raw_path)\n",
    "#     if '.edf' in hf.get_id(raw_path):\n",
    "#         # raw file\n",
    "#         raw = mne.io.read_raw_edf(raw_path)\n",
    "#         raw = create_eeg_montage(raw)\n",
    "#         data = rereference_channels(raw)\n",
    "        \n",
    "#         info = mne.create_info([MONTAGE_MAP[0], MONTAGE_MAP[21]], ch_types=['eeg']*2, sfreq=raw.info['sfreq'])\n",
    "#         raw = mne.io.RawArray(data, info)\n",
    "\n",
    "#         # transpose annotation list\n",
    "#         annots = list(zip(*annotations))\n",
    "#         # calculate durations\n",
    "#         durations = np.array(annots[2]).astype(float) - np.array(annots[1]).astype(float)\n",
    "#         # set annotations per channel\n",
    "#         raw = raw.set_annotations(mne.Annotations(onset=annots[1], duration=durations, description=annots[3], ch_names=[[x] for x in annots[0]]))\n",
    "\n",
    "\n",
    "# files += [raw, annotations]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/maligan/My Passport/msc_thesis/data/tuar/v2.1.0/edf/01_tcp_ar/107/00010748/s001_2013_09_19/00010748_s001_t000.rec_orig'"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hf.get_file_list(data_dir)\n",
    "# hf.get_file_list(subject)\n",
    "# hf.get_file_list(recording)\n",
    "hf.get_file_list(date)\n",
    "raw_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/media/maligan/My Passport/msc_thesis/data/tuar/v2_1_0/processed/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-314-fcfb0a79007d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdir_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/media/maligan/My Passport/msc_thesis/data/tuar/v2_1_0/processed/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_file_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdescriptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/VU/Year_2/M.Sc._Thesis_[X_400285]/my_thesis/code/ssl_thesis/helper_funcs.py\u001b[0m in \u001b[0;36mget_file_list\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# helper functions for loading TUH abnormal raw files from hierarchy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_file_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/media/maligan/My Passport/msc_thesis/data/tuar/v2_1_0/processed/'"
     ]
    }
   ],
   "source": [
    "\n",
    "dir_path = '/media/maligan/My Passport/msc_thesis/data/tuar/v2_1_0/processed/'\n",
    "\n",
    "files = hf.get_file_list(dir_path)\n",
    "dataset = []\n",
    "descriptions = []\n",
    "\n",
    "for i, f in enumerate(files):\n",
    "    dataset += [mne.io.read_raw_fif(f)]\n",
    "    descriptions += [{'subject': i}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = []\n",
    "\n",
    "for f in files:\n",
    "    for annot in mne.io.read_raw_fif(f).annotations:\n",
    "        annotations += [annot['description']]\n",
    "    \n",
    "# mne.io.read_raw_fif(files[0])._annotations[0]['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['bckg', 'chew', 'chew_elec', 'chew_musc', 'elec', 'eyem',\n",
       "       'eyem_chew', 'eyem_elec', 'eyem_musc', 'eyem_shiv', 'musc',\n",
       "       'musc_elec', 'shiv'], dtype='<U9')"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TUAR segmenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
