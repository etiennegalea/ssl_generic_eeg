{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "%matplotlib inline"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Self-supervised learning on EEG with relative positioning\n",
        "\n",
        "This example shows how to train a neural network with self-supervision on sleep\n",
        "EEG data. We follow the relative positioning approach of [1]_ on the openly\n",
        "accessible Sleep Physionet dataset [2]_ [3]_.\n",
        "\n",
        ".. topic:: Self-supervised learning\n",
        "\n",
        "    Self-supervised learning (SSL) is a learning paradigm that leverages\n",
        "    unlabelled data to train neural networks. First, neural networks are\n",
        "    trained on a \"pretext task\" which uses unlabelled data only. The pretext\n",
        "    task is designed based on a prior understanding of the data under study\n",
        "    (e.g., EEG has an underlying autocorrelation struture) and such that the\n",
        "    processing required to perform well on this pretext task is related to the\n",
        "    processing required to perform well on another task of interest.\n",
        "    Once trained, these neural networks can be reused as feature extractors or\n",
        "    weight initialization in a \"downstream task\", which is the task that we are\n",
        "    actually interested in (e.g., sleep staging). The pretext task step can\n",
        "    help reduce the quantity of labelled data needed to perform well on the\n",
        "    downstream task and/or improve downstream performance as compared to a\n",
        "    strictly supervised approach [1]_.\n",
        "\n",
        "Here, we use relative positioning (RP) as our pretext task, and perform sleep\n",
        "staging as our downstream task. RP is a simple SSL task, in which a neural\n",
        "network is trained to predict whether two randomly sampled EEG windows are\n",
        "close or far apart in time. This method was shown to yield physiologically- and\n",
        "clinically-relevant features and to boost classification performance in\n",
        "low-labels data regimes [1]_.\n",
        "   :depth: 2\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "# Authors: Hubert Banville <hubert.jbanville@gmail.com>\n",
        "#\n",
        "# License: BSD (3-clause)\n",
        "\n",
        "from braindecode.datasets.sleep_physionet import SleepPhysionet\n",
        "import pickle\n",
        "import numpy as np\n",
        "import mne\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "random_state = 87\n",
        "n_jobs = 1\n",
        "\n",
        "# sample data for which we are trying to generate predictions of the input data using a part of the SSL pre-trained model\n",
        "path_to_sample = \"/home/maligan/Documents/VU/Year_2/M.Sc._Thesis_[X_400285]/my_thesis/code/braindecode_code/sleep_staging_dataset/\""
      ],
      "outputs": [],
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading and preprocessing the dataset\n",
        "\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the raw recordings\n",
        "\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we load a few recordings from the Sleep Physionet dataset. Running\n",
        "this example with more recordings should yield better representations and\n",
        "downstream classification performance.\n",
        "\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "dataset = SleepPhysionet(\n",
        "    subject_ids=[0,1,2,3],\n",
        "    recording_ids=[1]\n",
        ")\n",
        "\n",
        "# from braindecode.datasets.tuh import TUHAbnormal\n",
        "\n",
        "# dataset = TUHAbnormal(\n",
        "#     path=\"/media/maligan/My Passport/msc thesis/tuh_abnormal_data/tuh_eeg_abnormal\"\n",
        "# )"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using default location ~/mne_data for PHYSIONET_SLEEP...\n",
            "Extracting EDF parameters from /home/maligan/mne_data/physionet-sleep-data/SC4001E0-PSG.edf...\n",
            "EDF file detected\n",
            "Setting channel info structure...\n",
            "Creating raw.info structure...\n",
            "Extracting EDF parameters from /home/maligan/mne_data/physionet-sleep-data/SC4011E0-PSG.edf...\n",
            "EDF file detected\n",
            "Setting channel info structure...\n",
            "Creating raw.info structure...\n",
            "Extracting EDF parameters from /home/maligan/mne_data/physionet-sleep-data/SC4021E0-PSG.edf...\n",
            "EDF file detected\n",
            "Setting channel info structure...\n",
            "Creating raw.info structure...\n",
            "Extracting EDF parameters from /home/maligan/mne_data/physionet-sleep-data/SC4031E0-PSG.edf...\n",
            "EDF file detected\n",
            "Setting channel info structure...\n",
            "Creating raw.info structure...\n"
          ]
        }
      ],
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing\n",
        "\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we preprocess the raw data. We convert the data to microvolts and apply\n",
        "a lowpass filter. Since the Sleep Physionet data is already sampled at 100 Hz\n",
        "we don't need to apply resampling.\n",
        "\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "source": [
        "from braindecode.datautil.preprocess import preprocess, Preprocessor\n",
        "\n",
        "high_cut_hz = 30\n",
        "\n",
        "preprocessors = [\n",
        "    Preprocessor(lambda x: x * 1e6),\n",
        "    Preprocessor('filter', l_freq=None, h_freq=high_cut_hz, n_jobs=n_jobs)\n",
        "]\n",
        "\n",
        "# Transform the data\n",
        "preprocess(dataset, preprocessors)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading 0 ... 2508000  =      0.000 ... 25080.000 secs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/home/maligan/Documents/VU/Year_2/M.Sc._Thesis_[X_400285]/my_thesis/code/braindecode_code/braindecode/braindecode/datautil/preprocess.py:10: UserWarning: datautil.preprocess module is deprecated and is now under preprocessing.preprocess, please use from import braindecode.preprocessing.preprocess\n",
            "  warn('datautil.preprocess module is deprecated and is now under '\n",
            "/home/maligan/Documents/VU/Year_2/M.Sc._Thesis_[X_400285]/my_thesis/code/braindecode_code/braindecode/braindecode/preprocessing/preprocess.py:52: UserWarning: Preprocessing choices with lambda functions cannot be saved.\n",
            "  warn('Preprocessing choices with lambda functions cannot be saved.')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtering raw data in 1 contiguous segment\n",
            "Setting up low-pass filter at 30 Hz\n",
            "\n",
            "FIR filter parameters\n",
            "---------------------\n",
            "Designing a one-pass, zero-phase, non-causal lowpass filter:\n",
            "- Windowed time-domain design (firwin) method\n",
            "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
            "- Upper passband edge: 30.00 Hz\n",
            "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
            "- Filter length: 45 samples (0.450 sec)\n",
            "\n",
            "Reading 0 ... 3261000  =      0.000 ... 32610.000 secs...\n",
            "Filtering raw data in 1 contiguous segment\n",
            "Setting up low-pass filter at 30 Hz\n",
            "\n",
            "FIR filter parameters\n",
            "---------------------\n",
            "Designing a one-pass, zero-phase, non-causal lowpass filter:\n",
            "- Windowed time-domain design (firwin) method\n",
            "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
            "- Upper passband edge: 30.00 Hz\n",
            "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
            "- Filter length: 45 samples (0.450 sec)\n",
            "\n",
            "Reading 0 ... 3060000  =      0.000 ... 30600.000 secs...\n",
            "Filtering raw data in 1 contiguous segment\n",
            "Setting up low-pass filter at 30 Hz\n",
            "\n",
            "FIR filter parameters\n",
            "---------------------\n",
            "Designing a one-pass, zero-phase, non-causal lowpass filter:\n",
            "- Windowed time-domain design (firwin) method\n",
            "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
            "- Upper passband edge: 30.00 Hz\n",
            "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
            "- Filter length: 45 samples (0.450 sec)\n",
            "\n",
            "Reading 0 ... 2850000  =      0.000 ... 28500.000 secs...\n",
            "Filtering raw data in 1 contiguous segment\n",
            "Setting up low-pass filter at 30 Hz\n",
            "\n",
            "FIR filter parameters\n",
            "---------------------\n",
            "Designing a one-pass, zero-phase, non-causal lowpass filter:\n",
            "- Windowed time-domain design (firwin) method\n",
            "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
            "- Upper passband edge: 30.00 Hz\n",
            "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
            "- Filter length: 45 samples (0.450 sec)\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<braindecode.datasets.sleep_physionet.SleepPhysionet at 0x7fe163fe1250>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extracting windows\n",
        "\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "We extract 30-s windows to be used in both the pretext and downstream tasks.\n",
        "As RP (and SSL in general) don't require labelled data, the pretext task\n",
        "could be performed using unlabelled windows extracted with\n",
        ":func:`braindecode.datautil.windower.create_fixed_length_window`.\n",
        "Here however, purely for convenience, we directly extract labelled windows so\n",
        "that we can reuse them in the sleep staging downstream task later.\n",
        "\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "source": [
        "from braindecode.datautil.windowers import create_windows_from_events\n",
        "\n",
        "window_size_s = 30\n",
        "sfreq = 100\n",
        "window_size_samples = window_size_s * sfreq\n",
        "\n",
        "mapping = {  # We merge stages 3 and 4 following AASM standards.\n",
        "    'Sleep stage W': 0,\n",
        "    'Sleep stage 1': 1,\n",
        "    'Sleep stage 2': 2,\n",
        "    'Sleep stage 3': 3,\n",
        "    'Sleep stage 4': 3,\n",
        "    'Sleep stage R': 4\n",
        "}\n",
        "\n",
        "windows_dataset = create_windows_from_events(\n",
        "    dataset, trial_start_offset_samples=0, trial_stop_offset_samples=0,\n",
        "    window_size_samples=window_size_samples,\n",
        "    window_stride_samples=window_size_samples, preload=True, mapping=mapping)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Used Annotations descriptions: ['Sleep stage 1', 'Sleep stage 2', 'Sleep stage 3', 'Sleep stage 4', 'Sleep stage R', 'Sleep stage W']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "837 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 837 events and 3000 original time points ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/home/maligan/Documents/VU/Year_2/M.Sc._Thesis_[X_400285]/my_thesis/code/braindecode_code/braindecode/braindecode/datautil/windowers.py:4: UserWarning: datautil.windowers module is deprecated and is now under preprocessing.windowers, please use from import braindecode.preprocessing.windowers\n",
            "  warn('datautil.windowers module is deprecated and is now under '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['Sleep stage 1', 'Sleep stage 2', 'Sleep stage 3', 'Sleep stage 4', 'Sleep stage R', 'Sleep stage W']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "1088 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 1088 events and 3000 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['Sleep stage 1', 'Sleep stage 2', 'Sleep stage 3', 'Sleep stage 4', 'Sleep stage R', 'Sleep stage W']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "1021 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 1021 events and 3000 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['Sleep stage 1', 'Sleep stage 2', 'Sleep stage 3', 'Sleep stage 4', 'Sleep stage R', 'Sleep stage W']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "951 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 951 events and 3000 original time points ...\n",
            "0 bad epochs dropped\n"
          ]
        }
      ],
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing windows\n",
        "\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also preprocess the windows by applying channel-wise z-score normalization.\n",
        "\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "source": [
        "from braindecode.datautil.preprocess import zscore\n",
        "\n",
        "preprocess(windows_dataset, [Preprocessor(zscore)])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/home/maligan/.local/lib/python3.8/site-packages/sklearn/utils/deprecation.py:86: FutureWarning: Function zscore is deprecated; will be removed in 0.7.0. Use sklearn.preprocessing.scale instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<braindecode.datasets.base.BaseConcatDataset at 0x7fe0c88a4d60>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Splitting dataset into train, valid and test sets\n",
        "\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "We randomly split the recordings by subject into train, validation and\n",
        "testing sets. We further define a new Dataset class which can receive a pair\n",
        "of indices and return the corresponding windows. This will be needed when\n",
        "training and evaluating on the pretext task.\n",
        "\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from braindecode.datasets import BaseConcatDataset\n",
        "\n",
        "subjects = np.unique(windows_dataset.description['subject'])\n",
        "subj_train, subj_test = train_test_split(\n",
        "    subjects, test_size=0.4, random_state=random_state)\n",
        "subj_valid, subj_test = train_test_split(\n",
        "    subj_test, test_size=0.5, random_state=random_state)\n",
        "\n",
        "\n",
        "class RelativePositioningDataset(BaseConcatDataset):\n",
        "    \"\"\"BaseConcatDataset with __getitem__ that expects 2 indices and a target.\n",
        "    \"\"\"\n",
        "    def __init__(self, list_of_ds):\n",
        "        super().__init__(list_of_ds)\n",
        "        self.return_pair = True\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if self.return_pair:\n",
        "            ind1, ind2, y = index\n",
        "            return (super().__getitem__(ind1)[0],\n",
        "                    super().__getitem__(ind2)[0]), y\n",
        "        else:\n",
        "            return super().__getitem__(index)\n",
        "\n",
        "    @property\n",
        "    def return_pair(self):\n",
        "        return self._return_pair\n",
        "\n",
        "    @return_pair.setter\n",
        "    def return_pair(self, value):\n",
        "        self._return_pair = value\n",
        "\n",
        "\n",
        "split_ids = {'train': subj_train, 'valid': subj_valid, 'test': subj_test}\n",
        "splitted = dict()\n",
        "for name, values in split_ids.items():\n",
        "    splitted[name] = RelativePositioningDataset(\n",
        "        [ds for ds in windows_dataset.datasets\n",
        "         if ds.description['subject'] in values])"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating samplers\n",
        "\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we need to create samplers. These samplers will be used to randomly\n",
        "sample pairs of examples to train and validate our model with\n",
        "self-supervision.\n",
        "\n",
        "The RP samplers have two main hyperparameters. `tau_pos` and `tau_neg`\n",
        "control the size of the \"positive\" and \"negative\" contexts, respectively.\n",
        "Pairs of windows that are separated by less than `tau_pos` samples will be\n",
        "given a label of `1`, while pairs of windows that are separated by more than\n",
        "`tau_neg` samples will be given a label of `0`. Here, we use the same values\n",
        "as in [1]_, i.e., `tau_pos`= 1 min and `tau_neg`= 15 mins.\n",
        "\n",
        "The samplers also control the number of pairs to be sampled (defined with\n",
        "`n_examples`). This number can be large to help regularize the pretext task\n",
        "training, for instance 2,000 pairs per recording as in [1]_. Here, we use a\n",
        "lower number of 250 pairs per recording to reduce training time.\n",
        "\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "source": [
        "from braindecode.samplers.ssl import RelativePositioningSampler\n",
        "\n",
        "tau_pos, tau_neg = int(sfreq * 60), int(sfreq * 15 * 60)\n",
        "n_examples_train = 250 * len(splitted['train'].datasets)\n",
        "n_examples_valid = 250 * len(splitted['valid'].datasets)\n",
        "n_examples_test = 250 * len(splitted['test'].datasets)\n",
        "\n",
        "train_sampler = RelativePositioningSampler(\n",
        "    splitted['train'].get_metadata(), tau_pos=tau_pos, tau_neg=tau_neg,\n",
        "    n_examples=n_examples_train, same_rec_neg=True, random_state=random_state)\n",
        "valid_sampler = RelativePositioningSampler(\n",
        "    splitted['valid'].get_metadata(), tau_pos=tau_pos, tau_neg=tau_neg,\n",
        "    n_examples=n_examples_valid, same_rec_neg=True,\n",
        "    random_state=random_state).presample()\n",
        "test_sampler = RelativePositioningSampler(\n",
        "    splitted['test'].get_metadata(), tau_pos=tau_pos, tau_neg=tau_neg,\n",
        "    n_examples=n_examples_test, same_rec_neg=True,\n",
        "    random_state=random_state).presample()"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "source": [
        "splitted['train'].get_metadata()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>i_window_in_trial</th>\n",
              "      <th>i_start_in_trial</th>\n",
              "      <th>i_stop_in_trial</th>\n",
              "      <th>target</th>\n",
              "      <th>subject</th>\n",
              "      <th>recording</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1974000</td>\n",
              "      <td>1977000</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1977000</td>\n",
              "      <td>1980000</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>1980000</td>\n",
              "      <td>1983000</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>1983000</td>\n",
              "      <td>1986000</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>1986000</td>\n",
              "      <td>1989000</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1016</th>\n",
              "      <td>51</td>\n",
              "      <td>5055000</td>\n",
              "      <td>5058000</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1017</th>\n",
              "      <td>52</td>\n",
              "      <td>5058000</td>\n",
              "      <td>5061000</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1018</th>\n",
              "      <td>53</td>\n",
              "      <td>5061000</td>\n",
              "      <td>5064000</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1019</th>\n",
              "      <td>54</td>\n",
              "      <td>5064000</td>\n",
              "      <td>5067000</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1020</th>\n",
              "      <td>55</td>\n",
              "      <td>5064001</td>\n",
              "      <td>5067001</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2109 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      i_window_in_trial  i_start_in_trial  i_stop_in_trial  target  subject  \\\n",
              "0                     0           1974000          1977000       0        1   \n",
              "1                     1           1977000          1980000       0        1   \n",
              "2                     2           1980000          1983000       0        1   \n",
              "3                     3           1983000          1986000       0        1   \n",
              "4                     4           1986000          1989000       0        1   \n",
              "...                 ...               ...              ...     ...      ...   \n",
              "1016                 51           5055000          5058000       0        2   \n",
              "1017                 52           5058000          5061000       0        2   \n",
              "1018                 53           5061000          5064000       0        2   \n",
              "1019                 54           5064000          5067000       0        2   \n",
              "1020                 55           5064001          5067001       0        2   \n",
              "\n",
              "      recording  \n",
              "0             1  \n",
              "1             1  \n",
              "2             1  \n",
              "3             1  \n",
              "4             1  \n",
              "...         ...  \n",
              "1016          1  \n",
              "1017          1  \n",
              "1018          1  \n",
              "1019          1  \n",
              "1020          1  \n",
              "\n",
              "[2109 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating the model\n",
        "\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now create the deep learning model. In this tutorial, we use a\n",
        "modified version of the sleep staging architecture introduced in [4]_ -\n",
        "a four-layer convolutional neural network - as our embedder.\n",
        "We change the dimensionality of the last layer to obtain a 100-dimension\n",
        "embedding, use 16 convolutional channels instead of 8, and add batch\n",
        "normalization after both temporal convolution layers.\n",
        "\n",
        "We further wrap the model into a siamese architecture using the\n",
        "# :class:`ContrastiveNet` class defined below. This allows us to train the\n",
        "feature extractor end-to-end.\n",
        "\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from braindecode.util import set_random_seeds\n",
        "from braindecode.models import SleepStagerChambon2018\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "if device == 'cuda':\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    print(\"CUDA enabled\")\n",
        "# Set random seed to be able to reproduce results\n",
        "set_random_seeds(seed=random_state, cuda=device == 'cuda')\n",
        "\n",
        "# Extract number of channels and time steps from dataset\n",
        "n_channels, input_size_samples = windows_dataset[0][0].shape\n",
        "emb_size = 100\n",
        "\n",
        "emb = SleepStagerChambon2018(\n",
        "    n_channels,\n",
        "    sfreq,\n",
        "    n_classes=emb_size,\n",
        "    n_conv_chs=16,\n",
        "    input_size_s=input_size_samples / sfreq,\n",
        "    dropout=0,\n",
        "    apply_batch_norm=True\n",
        ")\n",
        "\n",
        "class ContrastiveNet(nn.Module):\n",
        "    \"\"\"Contrastive module with linear layer on top of siamese embedder.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    emb : nn.Module\n",
        "        Embedder architecture.\n",
        "    emb_size : int\n",
        "        Output size of the embedder.\n",
        "    dropout : float\n",
        "        Dropout rate applied to the linear layer of the contrastive module.\n",
        "    \"\"\"\n",
        "    def __init__(self, emb, emb_size, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.emb = emb\n",
        "        self.clf = nn.Sequential(\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(emb_size, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1, x2 = x\n",
        "        z1, z2 = self.emb(x1), self.emb(x2)\n",
        "        return self.clf(torch.abs(z1 - z2)).flatten()\n",
        "\n",
        "\n",
        "model = ContrastiveNet(emb, emb_size).to(device)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/home/maligan/.local/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ]
        }
      ],
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "dataset[0]"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now train our network on the pretext task. We use similar hyperparameters as in 1, but reduce the number of epochs and increase the learning rate to account for the smaller setting of this example."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "source": [
        "import os\n",
        "\n",
        "from skorch.helper import predefined_split\n",
        "from skorch.callbacks import Checkpoint, EarlyStopping, EpochScoring\n",
        "from braindecode import EEGClassifier\n",
        "\n",
        "lr = 5e-3\n",
        "batch_size = 512\n",
        "# for the sake of testing, reduce epochs to just 3 for now\n",
        "# n_epochs = 25\n",
        "n_epochs = 3\n",
        "num_workers = 0 if n_jobs <= 1 else n_jobs\n",
        "\n",
        "cp = Checkpoint(dirname='', f_criterion=None, f_optimizer=None, f_history=None)\n",
        "early_stopping = EarlyStopping(patience=10)\n",
        "train_acc = EpochScoring(\n",
        "    scoring='accuracy', on_train=True, name='train_acc', lower_is_better=False)\n",
        "valid_acc = EpochScoring(\n",
        "    scoring='accuracy', on_train=False, name='valid_acc',\n",
        "    lower_is_better=False)\n",
        "callbacks = [\n",
        "    ('cp', cp),\n",
        "    ('patience', early_stopping),\n",
        "    ('train_acc', train_acc),\n",
        "    ('valid_acc', valid_acc)\n",
        "]\n",
        "\n",
        "clf = EEGClassifier(\n",
        "    model,\n",
        "    criterion=torch.nn.BCEWithLogitsLoss,\n",
        "    optimizer=torch.optim.Adam,\n",
        "    max_epochs=n_epochs,\n",
        "    iterator_train__shuffle=False,\n",
        "    iterator_train__sampler=train_sampler,\n",
        "    iterator_valid__sampler=valid_sampler,\n",
        "    iterator_train__num_workers=num_workers,\n",
        "    iterator_valid__num_workers=num_workers,\n",
        "    train_split=predefined_split(splitted['valid']),\n",
        "    optimizer__lr=lr,\n",
        "    batch_size=batch_size,\n",
        "    callbacks=callbacks,\n",
        "    device=device\n",
        ")\n",
        "# Model training for a specified number of epochs. `y` is None as it is already\n",
        "# supplied in the dataset.\n",
        "clf.fit(splitted['train'], y=None)\n",
        "clf.load_params(checkpoint=cp)  # Load the model with the lowest valid_loss\n",
        "\n",
        "os.remove('./params.pt')  # Delete parameters file\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  epoch    train_acc    train_loss    valid_acc    valid_loss    cp     dur\n",
            "-------  -----------  ------------  -----------  ------------  ----  ------\n",
            "      1       \u001b[36m0.5180\u001b[0m        \u001b[32m0.7080\u001b[0m       \u001b[35m0.6080\u001b[0m        \u001b[31m0.6825\u001b[0m     +  7.8549\n",
            "      2       0.5000        0.7200       \u001b[35m0.6440\u001b[0m        \u001b[31m0.6754\u001b[0m     +  7.0301\n",
            "      3       \u001b[36m0.5880\u001b[0m        \u001b[32m0.6784\u001b[0m       \u001b[35m0.6720\u001b[0m        \u001b[31m0.6469\u001b[0m     +  6.5983\n"
          ]
        }
      ],
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing the results\n",
        "\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inspecting pretext task performance\n",
        "\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "We plot the loss and pretext task performance for the training and validation\n",
        "sets.\n",
        "\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.lines import Line2D\n",
        "import pandas as pd\n",
        "\n",
        "# Extract loss and balanced accuracy values for plotting from history object\n",
        "df = pd.DataFrame(clf.history.to_list())\n",
        "\n",
        "df['train_acc'] *= 100\n",
        "df['valid_acc'] *= 100\n",
        "\n",
        "ys1 = ['train_loss', 'valid_loss']\n",
        "ys2 = ['train_acc', 'valid_acc']\n",
        "styles = ['-', ':']\n",
        "markers = ['.', '.']\n",
        "\n",
        "plt.style.use('seaborn-talk')\n",
        "\n",
        "fig, ax1 = plt.subplots(figsize=(8, 3))\n",
        "ax2 = ax1.twinx()\n",
        "for y1, y2, style, marker in zip(ys1, ys2, styles, markers):\n",
        "    ax1.plot(df['epoch'], df[y1], ls=style, marker=marker, ms=7,\n",
        "             c='tab:blue', label=y1)\n",
        "    ax2.plot(df['epoch'], df[y2], ls=style, marker=marker, ms=7,\n",
        "             c='tab:orange', label=y2)\n",
        "\n",
        "ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
        "ax1.set_ylabel('Loss', color='tab:blue')\n",
        "ax2.tick_params(axis='y', labelcolor='tab:orange')\n",
        "ax2.set_ylabel('Accuracy [%]', color='tab:orange')\n",
        "ax1.set_xlabel('Epoch')\n",
        "\n",
        "lines1, labels1 = ax1.get_legend_handles_labels()\n",
        "lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "ax2.legend(lines1 + lines2, labels1 + labels2)\n",
        "\n",
        "plt.tight_layout()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 576x216 with 2 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also display the confusion matrix and classification report for the\n",
        "pretext task:\n",
        "\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Switch to the test sampler\n",
        "clf.iterator_valid__sampler = test_sampler\n",
        "y_pred = clf.forward(splitted['test'], training=False) > 0\n",
        "y_true = [y for _, _, y in test_sampler]\n",
        "\n",
        "print(confusion_matrix(y_true, y_pred))\n",
        "print(classification_report(y_true, y_pred))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 66  55]\n",
            " [ 23 106]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.74      0.55      0.63       121\n",
            "         1.0       0.66      0.82      0.73       129\n",
            "\n",
            "    accuracy                           0.69       250\n",
            "   macro avg       0.70      0.68      0.68       250\n",
            "weighted avg       0.70      0.69      0.68       250\n",
            "\n"
          ]
        }
      ],
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "source": [
        "# fitting raw EEG data to the modl\n",
        "\n",
        "X = mne.io.read_raw_fif(path_to_sample+\"0-raw.fif\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Opening raw data file /home/maligan/Documents/VU/Year_2/M.Sc._Thesis_[X_400285]/my_thesis/code/braindecode_code/sleep_staging_dataset/0-raw.fif...\n",
            "Isotrak not found\n",
            "    Range : 2883000 ... 5391000 =  28830.000 ... 53910.000 secs\n",
            "Ready.\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "source": [
        "X_df = X.to_data_frame()\n",
        "# X_arr = np.array(X.to_data_frame())\n",
        "X_list = list(zip(X_df['Fpz-Cz'], X_df['Pz-Oz']))\n",
        "X_tensor = torch.Tensor(X_list)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "source": [
        "emb = model.get_submodule(\"emb\")\n",
        "emb_fe = model.get_submodule(\"emb.feature_extractor\")"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "source": [
        "# embedder submodule - we want to remove feature classification and just use the nodes witht he convolutions, batch normalization and max pooling\n",
        "# scope is to see what the pre-trained model comes up with\n",
        "\n",
        "print(type(model))\n",
        "model"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class '__main__.ContrastiveNet'>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ContrastiveNet(\n",
              "  (emb): SleepStagerChambon2018(\n",
              "    (spatial_conv): Conv2d(1, 2, kernel_size=(2, 1), stride=(1, 1))\n",
              "    (feature_extractor): Sequential(\n",
              "      (0): Conv2d(1, 16, kernel_size=(1, 50), stride=(1, 1), padding=(0, 25))\n",
              "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "      (3): MaxPool2d(kernel_size=(1, 13), stride=(1, 13), padding=0, dilation=1, ceil_mode=False)\n",
              "      (4): Conv2d(16, 16, kernel_size=(1, 50), stride=(1, 1), padding=(0, 25))\n",
              "      (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (6): ReLU()\n",
              "      (7): MaxPool2d(kernel_size=(1, 13), stride=(1, 13), padding=0, dilation=1, ceil_mode=False)\n",
              "    )\n",
              "    (fc): Sequential(\n",
              "      (0): Dropout(p=0, inplace=False)\n",
              "      (1): Linear(in_features=544, out_features=100, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (clf): Sequential(\n",
              "    (0): Dropout(p=0.5, inplace=False)\n",
              "    (1): Linear(in_features=100, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "source": [
        "# Does not work:\n",
        "# Data is in wrong format OR it does not satisfy arguments of .forward() function\n",
        "# still figuring this one out...\n",
        "X[0]\n",
        "x_1 = torch.from_numpy(X[0][0])\n",
        "x_2 = torch.from_numpy(X[0][1])"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "source": [
        "# model([x_1, x_2])\n",
        "# emb(torch.randn(2, 1000))\n",
        "\n",
        "N, D_in, H, D_out = 64, 1000, 100, 10\n",
        "\n",
        "x = torch.randn(N, D_in)\n",
        "y = torch.randn(N, D_out)\n",
        "\n",
        "_model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(D_in, H),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(H, D_out),\n",
        ")"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "source": [
        "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
        "\n",
        "learning_rate = 1e-4\n",
        "for t in range(500):\n",
        "    # Forward pass: compute predicted y by passing x to the model. Module objects\n",
        "    # override the __call__ operator so you can call them like functions. When\n",
        "    # doing so you pass a Tensor of input data to the Module and it produces\n",
        "    # a Tensor of output data.\n",
        "    y_pred = _model(x)\n",
        "\n",
        "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
        "    # values of y, and the loss function returns a Tensor containing the\n",
        "    # loss.\n",
        "    loss = loss_fn(y_pred, y)\n",
        "    if t % 100 == 99:\n",
        "        print(t, loss.item())\n",
        "\n",
        "    # Zero the gradients before running the backward pass.\n",
        "    _model.zero_grad()\n",
        "\n",
        "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
        "    # parameters of the _model. Internally, the parameters of each Module are stored\n",
        "    # in Tensors with requires_grad=True, so this call will compute gradients for\n",
        "    # all learnable parameters in the _model.\n",
        "    loss.backward()\n",
        "\n",
        "    # Update the weights using gradient descent. Each parameter is a Tensor, so\n",
        "    # we can access its gradients like we did before.\n",
        "    with torch.no_grad():\n",
        "        for param in _model.parameters():\n",
        "            param -= learning_rate * param.grad"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "99 2.4183855056762695\n",
            "199 0.033818475902080536\n",
            "299 0.0012862526345998049\n",
            "399 6.936051067896187e-05\n",
            "499 4.408575023262529e-06\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "source": [
        "# model.emb(torch.from_numpy(X[0][0]))\n",
        "\n",
        "_data = mne.io.read_raw_edf(\"/home/maligan/mne_data/physionet-sleep-data/SC4001E0-PSG.edf\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting EDF parameters from /home/maligan/mne_data/physionet-sleep-data/SC4001E0-PSG.edf...\n",
            "EDF file detected\n",
            "Setting channel info structure...\n",
            "Creating raw.info structure...\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "source": [
        "X = SleepPhysionet(\n",
        "    subject_ids=[0],\n",
        "    recording_ids=[1]\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using default location ~/mne_data for PHYSIONET_SLEEP...\n",
            "Extracting EDF parameters from /home/maligan/mne_data/physionet-sleep-data/SC4001E0-PSG.edf...\n",
            "EDF file detected\n",
            "Setting channel info structure...\n",
            "Creating raw.info structure...\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "source": [
        "\n",
        "X_windowed = create_windows_from_events(\n",
        "    X, trial_start_offset_samples=0, trial_stop_offset_samples=0,\n",
        "    window_size_samples=window_size_samples,\n",
        "    window_stride_samples=window_size_samples)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Used Annotations descriptions: ['Sleep stage 1', 'Sleep stage 2', 'Sleep stage 3', 'Sleep stage 4', 'Sleep stage R', 'Sleep stage W']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "837 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 837 events and 3000 original time points ...\n",
            "0 bad epochs dropped\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "source": [
        "def print_dataset_stats(pre_train_data, input_data):\n",
        "    print(\"- pre-training data -\")\n",
        "    print(type(pre_train_data))\n",
        "    print(\"O:\", len(pre_train_data))\n",
        "    print(\"1:\", len(pre_train_data[0]))\n",
        "    print(\"2:\", len(pre_train_data[0][0]))\n",
        "    print(\"3:\", len(pre_train_data[0][0][0]))\n",
        "\n",
        "    print(\"------------------------------------------------------------------\")\n",
        "\n",
        "    print(\"- input data -\")\n",
        "    print(type(input_data))\n",
        "    print(\"O:\", len(input_data))\n",
        "    print(\"1:\", len(input_data[0]))\n",
        "    print(\"2:\", len(input_data[0][0]))\n",
        "    print(\"3:\", len(input_data[0][0][0]))"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "source": [
        "print_dataset_stats(dataset, _data)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- pre-training data -\n",
            "<class 'braindecode.datasets.sleep_physionet.SleepPhysionet'>\n",
            "O: 11679004\n",
            "1: 2\n",
            "2: 2\n",
            "3: 1\n",
            "------------------------------------------------------------------\n",
            "- input data -\n",
            "<class 'mne.io.edf.edf.RawEDF'>\n",
            "O: 7950000\n",
            "1: 2\n",
            "2: 1\n",
            "3: 7950000\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "source": [
        "torch.from_numpy(_data)"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "expected np.ndarray (got RawEDF)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-95-acce83e5c194>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: expected np.ndarray (got RawEDF)"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "source": [
        "from braindecode.datautil.windowers import create_windows_from_events\n",
        "\n",
        "\n",
        "_data_windows = create_windows_from_events(\n",
        "    _data, trial_start_offset_samples=0, trial_stop_offset_samples=0,\n",
        "    window_size_samples=window_size_samples,\n",
        "    window_stride_samples=window_size_samples,\n",
        "    preload=True\n",
        "    )\n"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'RawEDF' object has no attribute 'datasets'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-e72a955643eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m _data_windows = create_windows_from_events(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0m_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial_start_offset_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial_stop_offset_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mwindow_size_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow_size_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Documents/VU/Year_2/M.Sc._Thesis_[X_400285]/my_thesis/code/braindecode_code/braindecode/braindecode/preprocessing/windowers.py\u001b[0m in \u001b[0;36mcreate_windows_from_events\u001b[0;34m(concat_ds, trial_start_offset_samples, trial_stop_offset_samples, window_size_samples, window_stride_samples, drop_last_window, mapping, preload, drop_bad_windows, picks, reject, flat, on_missing, n_jobs)\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mwindow_size_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_stride_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_last_window\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0mmapping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_bad_windows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpicks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             on_missing) for ds in concat_ds.datasets)\n\u001b[0m\u001b[1;32m    119\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mBaseConcatDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_of_windows_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'RawEDF' object has no attribute 'datasets'"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "source": [],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "source": [],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11679004"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "################################################################################################################################################"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "source": [],
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Expected 4-dimensional input for 4-dimensional weight [2, 1, 2, 1], but got 2-dimensional input of size [2508001, 2] instead",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-180-a9dbfc4fde6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0memb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Documents/VU/Year_2/M.Sc._Thesis_[X_400285]/my_thesis/code/braindecode_code/braindecode/braindecode/models/sleep_stager_chambon_2018.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_channels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspatial_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    437\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 439\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    440\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected 4-dimensional input for 4-dimensional weight [2, 1, 2, 1], but got 2-dimensional input of size [2508001, 2] instead"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "source": [
        "X_arr.ndim"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 167
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using the learned representation for sleep staging\n",
        "\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now use the trained convolutional neural network as a feature\n",
        "extractor. We perform sleep stage classification from the learned feature\n",
        "representation using a linear logistic regression classifier.\n",
        "\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Extract features with the trained embedder\n",
        "data = dict()\n",
        "for name, split in splitted.items():\n",
        "    split.return_pair = False  # Return single windows\n",
        "    loader = DataLoader(split, batch_size=batch_size, num_workers=num_workers)\n",
        "    with torch.no_grad():\n",
        "        feats = [emb(batch_x.to(device)).cpu().numpy()\n",
        "                 for batch_x, _, _ in loader]\n",
        "    data[name] = (np.concatenate(feats), split.get_metadata()['target'].values)\n",
        "\n",
        "# Initialize the logistic regression model\n",
        "log_reg = LogisticRegression(\n",
        "    penalty='l2', C=1.0, class_weight='balanced', solver='lbfgs',\n",
        "    multi_class='multinomial', random_state=random_state)\n",
        "clf_pipe = make_pipeline(StandardScaler(), log_reg)\n",
        "\n",
        "# Fit and score the logistic regression\n",
        "clf_pipe.fit(*data['train'])\n",
        "train_y_pred = clf_pipe.predict(data['train'][0])\n",
        "valid_y_pred = clf_pipe.predict(data['valid'][0])\n",
        "test_y_pred = clf_pipe.predict(data['test'][0])\n",
        "\n",
        "train_bal_acc = balanced_accuracy_score(data['train'][1], train_y_pred)\n",
        "valid_bal_acc = balanced_accuracy_score(data['valid'][1], valid_y_pred)\n",
        "test_bal_acc = balanced_accuracy_score(data['test'][1], test_y_pred)\n",
        "\n",
        "print('Sleep staging performance with logistic regression:')\n",
        "print(f'Train bal acc: {train_bal_acc:0.4f}')\n",
        "print(f'Valid bal acc: {valid_bal_acc:0.4f}')\n",
        "print(f'Test bal acc: {test_bal_acc:0.4f}')\n",
        "\n",
        "print('Results on test set:')\n",
        "print(confusion_matrix(data['test'][1], test_y_pred))\n",
        "print(classification_report(data['test'][1], test_y_pred))"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'str' object has no attribute 'decode'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-fe7ccc8fe281>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Fit and score the logistic regression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mclf_pipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mtrain_y_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf_pipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mvalid_y_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf_pipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'passthrough'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m                 \u001b[0mfit_params_last_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_params_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1405\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m             \u001b[0mprefer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'processes'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n\u001b[0m\u001b[1;32m   1408\u001b[0m                                \u001b[0;34m**\u001b[0m\u001b[0m_joblib_parallel_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprefer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m             path_func(X, y, pos_class=class_, Cs=[C_],\n",
            "\u001b[0;32m~/.local/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1039\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1041\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    857\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36m_logistic_regression_path\u001b[0;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio)\u001b[0m\n\u001b[1;32m    760\u001b[0m                 \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"iprint\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0miprint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"gtol\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"maxiter\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m             )\n\u001b[0;32m--> 762\u001b[0;31m             n_iter_i = _check_optimize_result(\n\u001b[0m\u001b[1;32m    763\u001b[0m                 \u001b[0msolver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m                 extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/optimize.py\u001b[0m in \u001b[0;36m_check_optimize_result\u001b[0;34m(solver, result, max_iter, extra_warning_msg)\u001b[0m\n\u001b[1;32m    241\u001b[0m                 \u001b[0;34m\"    https://scikit-learn.org/stable/modules/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m                 \u001b[0;34m\"preprocessing.html\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m             ).format(solver, result.status, result.message.decode(\"latin1\"))\n\u001b[0m\u001b[1;32m    244\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_warning_msg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m                 \u001b[0mwarning_msg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_warning_msg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'decode'"
          ]
        }
      ],
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The balanced accuracy is much higher than chance-level (i.e., 20% for our\n",
        "5-class classification problem). Finally, we perform a quick 2D visualization\n",
        "of the feature space using a PCA:\n",
        "\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from sklearn.decomposition import PCA\n",
        "# from sklearn.manifold import TSNE\n",
        "from matplotlib import cm\n",
        "\n",
        "X = np.concatenate([v[0] for k, v in data.items()])\n",
        "y = np.concatenate([v[1] for k, v in data.items()])\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "# tsne = TSNE(n_components=2)\n",
        "components = pca.fit_transform(X)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "colors = cm.get_cmap('viridis', 5)(range(5))\n",
        "for i, stage in enumerate(['W', 'N1', 'N2', 'N3', 'R']):\n",
        "    mask = y == i\n",
        "    ax.scatter(components[mask, 0], components[mask, 1], s=10, alpha=0.7,\n",
        "               color=colors[i], label=stage)\n",
        "ax.legend()"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that there is sleep stage-related structure in the embedding. A\n",
        "nonlinear projection method (e.g., tSNE, UMAP) might yield more insightful\n",
        "visualizations. Using a similar approach, the embedding space could also be\n",
        "explored with respect to subject-level features, e.g., age and sex.\n",
        "\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we used self-supervised learning (SSL) as a way to learn\n",
        "representations from unlabelled raw EEG data. Specifically, we used the\n",
        "relative positioning (RP) pretext task to train a feature extractor on a\n",
        "subset of the Sleep Physionet dataset. We then reused these features in a\n",
        "downstream sleep staging task. We achieved reasonable downstream performance\n",
        "and further showed with a 2D projection that the learned embedding space\n",
        "contained sleep-related structure.\n",
        "\n",
        "Many avenues could be taken to improve on these results. For instance, using\n",
        "the entire Sleep Physionet dataset or training on larger datasets should help\n",
        "the feature extractor learn better representations during the pretext task.\n",
        "Other SSL tasks such as those described in [1]_ could further help discover\n",
        "more powerful features.\n",
        "\n",
        "\n",
        "## References\n",
        "\n",
        ".. [1] Banville, H., Chehab, O., Hyvärinen, A., Engemann, D. A., & Gramfort, A.\n",
        "      (2020). Uncovering the structure of clinical EEG signals with\n",
        "      self-supervised learning. arXiv preprint arXiv:2007.16104.\n",
        "\n",
        ".. [2] Kemp, B., Zwinderman, A. H., Tuk, B., Kamphuisen, H. A., & Oberye, J. J.\n",
        "       (2000). Analysis of a sleep-dependent neuronal feedback loop: the\n",
        "       slow-wave microcontinuity of the EEG. IEEE Transactions on Biomedical\n",
        "       Engineering, 47(9), 1185-1194.\n",
        "\n",
        ".. [3] Goldberger, A. L., Amaral, L. A., Glass, L., Hausdorff, J. M., Ivanov,\n",
        "       P. C., Mark, R. G., ... & Stanley, H. E. (2000). PhysioBank,\n",
        "       PhysioToolkit, and PhysioNet: components of a new research resource for\n",
        "       complex physiologic signals. circulation, 101(23), e215-e220.\n",
        "\n",
        ".. [4] Chambon, S., Galtier, M., Arnal, P., Wainrib, G. and Gramfort, A.\n",
        "      (2018)A Deep Learning Architecture for Temporal Sleep Stage\n",
        "      Classification Using Multivariate and Multimodal Time Series.\n",
        "      IEEE Trans. on Neural Systems and Rehabilitation Engineering 26:\n",
        "      (758-769)\n",
        "\n",
        "\n"
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.10 64-bit"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "interpreter": {
      "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}