{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Sleep staging on the Sleep Physionet dataset\n\nThis tutorial shows how to train and test a sleep staging neural network with\nBraindecode. We follow the approach of [1]_ on the openly accessible Sleep\nPhysionet dataset [1]_ [2]_.\n\n## References\n.. [1] Chambon, S., Galtier, M., Arnal, P., Wainrib, G. and Gramfort, A.\n      (2018)A Deep Learning Architecture for Temporal Sleep Stage\n      Classification Using Multivariate and Multimodal Time Series.\n      IEEE Trans. on Neural Systems and Rehabilitation Engineering 26:\n      (758-769)\n\n.. [2] B Kemp, AH Zwinderman, B Tuk, HAC Kamphuisen, JJL Obery\u00e9. Analysis of\n       a sleep-dependent neuronal feedback loop: the slow-wave\n       microcontinuity of the EEG. IEEE-BME 47(9):1185-1194 (2000).\n\n.. [3] Goldberger AL, Amaral LAN, Glass L, Hausdorff JM, Ivanov PCh,\n       Mark RG, Mietus JE, Moody GB, Peng C-K, Stanley HE. (2000)\n       PhysioBank, PhysioToolkit, and PhysioNet: Components of a New\n       Research Resource for Complex Physiologic Signals.\n       Circulation 101(23):e215-e220\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Authors: Hubert Banville <hubert.jbanville@gmail.com>\n#\n# License: BSD (3-clause)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading and preprocessing the dataset\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loading\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, we load the data using the\n:class:`braindecode.datasets.sleep_physionet.SleepPhysionet` class. We load\ntwo recordings from two different individuals: we will use the first one to\ntrain our network and the second one to evaluate performance (as in the `MNE`_\nsleep staging example).\n\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>To load your own datasets either via MNE or from\n   preprocessed X/y numpy arrays, see the `MNE Dataset\n   Tutorial <./plot_mne_dataset_example.html>`__ and the `Numpy Dataset\n   Tutorial <./plot_custom_dataset_example.html>`__.</p></div>\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from braindecode.datasets.sleep_physionet import SleepPhysionet\n\ndataset = SleepPhysionet(\n    subject_ids=[0, 1], recording_ids=[1], crop_wake_mins=30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preprocessing\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we preprocess the raw data. We apply convert the data to microvolts and\napply a lowpass filter. We omit the downsampling step of [1]_ as the Sleep\nPhysionet data is already sampled at a lower 100 Hz.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from braindecode.datautil.preprocess import preprocess, Preprocessor\n\nhigh_cut_hz = 30\n\npreprocessors = [\n    Preprocessor(lambda x: x * 1e6),\n    Preprocessor('filter', l_freq=None, h_freq=high_cut_hz)\n]\n\n# Transform the data\npreprocess(dataset, preprocessors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Extract windows\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We extract 30-s windows to be used in the classification task.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from braindecode.datautil.windowers import create_windows_from_events\n\n\nmapping = {  # We merge stages 3 and 4 following AASM standards.\n    'Sleep stage W': 0,\n    'Sleep stage 1': 1,\n    'Sleep stage 2': 2,\n    'Sleep stage 3': 3,\n    'Sleep stage 4': 3,\n    'Sleep stage R': 4\n}\n\nwindow_size_s = 30\nsfreq = 100\nwindow_size_samples = window_size_s * sfreq\n\nwindows_dataset = create_windows_from_events(\n    dataset, trial_start_offset_samples=0, trial_stop_offset_samples=0,\n    window_size_samples=window_size_samples,\n    window_stride_samples=window_size_samples, preload=True, mapping=mapping)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Window preprocessing\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We also preprocess the windows by applying channel-wise z-score normalization\nin each window.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from braindecode.datautil.preprocess import zscore\n\npreprocess(windows_dataset, [Preprocessor(zscore)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Split dataset into train and valid\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can easily split the dataset using additional info stored in the\n`description` attribute of :class:`braindecode.datasets.BaseDataset`,\nin this case using the ``subject`` column. Here, we split the examples per subject.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "splitted = windows_dataset.split('subject')\ntrain_set = splitted['0']\nvalid_set = splitted['1']\n\n# Print number of examples per class\nprint(train_set.datasets[0].windows)\nprint(valid_set.datasets[0].windows)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create model\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now create the deep learning model. In this tutorial, we use the sleep\nstaging architecture introduced in [1]_, which is a four-layer convolutional\nneural network.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nfrom braindecode.util import set_random_seeds\nfrom braindecode.models import SleepStagerChambon2018\n\ncuda = torch.cuda.is_available()  # check if GPU is available\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nif cuda:\n    torch.backends.cudnn.benchmark = True\n# Set random seed to be able to reproduce results\nset_random_seeds(seed=87, cuda=cuda)\n\nn_classes = 5\n# Extract number of channels and time steps from dataset\nn_channels = train_set[0][0].shape[0]\ninput_size_samples = train_set[0][0].shape[1]\n\nmodel = SleepStagerChambon2018(\n    n_channels,\n    sfreq,\n    n_classes=n_classes,\n    input_size_s=input_size_samples / sfreq\n)\n\n# Send model to GPU\nif cuda:\n    model.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now train our network. :class:`braindecode.EEGClassifier` is a\nbraindecode object that is responsible for managing the training of neural\nnetworks. It inherits from :class:`skorch.NeuralNetClassifier`, so the\ntraining logic is the same as in\n`Skorch <https://skorch.readthedocs.io/en/stable/>`__.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Note**: We use different hyperparameters from [1]_, as\nthese hyperparameters were optimized on a different dataset (MASS SS3) and\nwith a different number of recordings. Generally speaking, it is\nrecommended to perform hyperparameter optimization if reusing this code on\na different dataset or with more recordings.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from skorch.helper import predefined_split\nfrom skorch.callbacks import EpochScoring\nfrom braindecode import EEGClassifier\n\nlr = 5e-4\nbatch_size = 16\nn_epochs = 5\n\ntrain_bal_acc = EpochScoring(\n    scoring='balanced_accuracy', on_train=True, name='train_bal_acc',\n    lower_is_better=False)\nvalid_bal_acc = EpochScoring(\n    scoring='balanced_accuracy', on_train=False, name='valid_bal_acc',\n    lower_is_better=False)\ncallbacks = [('train_bal_acc', train_bal_acc),\n             ('valid_bal_acc', valid_bal_acc)]\n\nclf = EEGClassifier(\n    model,\n    criterion=torch.nn.CrossEntropyLoss,\n    optimizer=torch.optim.Adam,\n    train_split=predefined_split(valid_set),  # using valid_set for validation\n    optimizer__lr=lr,\n    batch_size=batch_size,\n    callbacks=callbacks,\n    device=device\n)\n# Model training for a specified number of epochs. `y` is None as it is already\n# supplied in the dataset.\nclf.fit(train_set, y=None, epochs=n_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot results\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We use the history stored by Skorch during training to plot the performance of\nthe model throughout training. Specifically, we plot the loss and the balanced\nmisclassification rate (1 - balanced accuracy) for the training and validation\nsets.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nfrom matplotlib.lines import Line2D\nimport pandas as pd\n\n# Extract loss and balanced accuracy values for plotting from history object\ndf = pd.DataFrame(clf.history.to_list())\ndf[['train_mis_clf', 'valid_mis_clf']] = 100 - df[\n    ['train_bal_acc', 'valid_bal_acc']] * 100\n\n# get percent of misclass for better visual comparison to loss\nplt.style.use('seaborn-talk')\nfig, ax1 = plt.subplots(figsize=(8, 3))\ndf.loc[:, ['train_loss', 'valid_loss']].plot(\n    ax=ax1, style=['-', ':'], marker='o', color='tab:blue', legend=False,\n    fontsize=14)\n\nax1.tick_params(axis='y', labelcolor='tab:blue', labelsize=14)\nax1.set_ylabel(\"Loss\", color='tab:blue', fontsize=14)\n\nax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n\ndf.loc[:, ['train_mis_clf', 'valid_mis_clf']].plot(\n    ax=ax2, style=['-', ':'], marker='o', color='tab:red', legend=False)\nax2.tick_params(axis='y', labelcolor='tab:red', labelsize=14)\nax2.set_ylabel('Balanced misclassification rate [%]', color='tab:red',\n               fontsize=14)\nax2.set_ylim(ax2.get_ylim()[0], 85)  # make some room for legend\nax1.set_xlabel('Epoch', fontsize=14)\n\n# where some data has already been plotted to ax\nhandles = []\nhandles.append(\n    Line2D([0], [0], color='black', linewidth=1, linestyle='-', label='Train'))\nhandles.append(\n    Line2D([0], [0], color='black', linewidth=1, linestyle=':', label='Valid'))\nplt.legend(handles, [h.get_label() for h in handles], fontsize=14)\nplt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we also display the confusion matrix and classification report:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\ny_true = valid_set.datasets[0].windows.metadata['target'].values\ny_pred = clf.predict(valid_set)\n\nprint(confusion_matrix(y_true, y_pred))\n\nprint(classification_report(y_true, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our model was able to perform reasonably well given the low amount of data\navailable, reaching a balanced accuracy of around 55% in a 5-class\nclassification task (chance-level = 20%) on held-out data.\n\nTo further improve performance, more recordings can be included in the\ntraining set, and various modifications can be made to the model (e.g.,\naggregating the representation of multiple consecutive windows [1]_).\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}